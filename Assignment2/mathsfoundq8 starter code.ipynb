{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043390e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GIVEN DATA\n",
      "======================================================================\n",
      "Size (s): [1000 1500 1200 2000 1800]\n",
      "Bedrooms (b): [2 3 2 4 3]\n",
      "Age (a): [ 5 10  2 20 15]\n",
      "Prices (y): [200 300 240 400 350]\n",
      "\n",
      "======================================================================\n",
      "PART (a): Design Matrix and Linear Dependence\n",
      "======================================================================\n",
      "Design Matrix X:\n",
      "[[   1 1000    2    5]\n",
      " [   1 1500    3   10]\n",
      " [   1 1200    2    2]\n",
      " [   1 2000    4   20]\n",
      " [   1 1800    3   15]]\n",
      "Shape of X: (5, 4)\n",
      "\n",
      "Rank of X: 4\n",
      "Number of columns: 4\n",
      "Are columns linearly independent? True\n",
      "\n",
      "Determinant of X^T X: 14860000.000000093\n",
      "\n",
      "Finding linear dependence relationship...\n",
      "Columns are linearly independent - no non-trivial linear combination exists\n",
      "\n",
      "TODO: Write your interpretation of what linear dependence means for ML models\n",
      "Practical Implication:\n",
      "\n",
      "Linear dependence means that the coefficient of one feature can be expressed as a combination of others, preventing unique solutions to a linear regression which then leads to infinitely many solutions. Also, models may become unstable and sensitive to small changes in data which can lead to overfitting. This is known as multicollinearity in regression analysis and this can be remedied by removing redundant features or using regularization techniques, or by applying PCA to reduce dimensionality.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PART (b): Gram-Schmidt Orthogonalization\n",
      "======================================================================\n",
      "Original feature vectors:\n",
      "v1 (size): [1000 1500 1200 2000 1800]\n",
      "v2 (bedrooms): [2 3 2 4 3]\n",
      "v3 (age): [ 5 10  2 20 15]\n",
      "\n",
      "Orthogonal basis vectors:\n",
      "u1: [1000 1500 1200 2000 1800]\n",
      "u2: [ 0.13  0.2  -0.24  0.26 -0.36]\n",
      "u3: [-3.28 -2.41 -5.54  3.45  3.69]\n",
      "\n",
      "Verification of orthogonality:\n",
      "u1 · u2: 0.000\n",
      "u1 · u3: 0.000\n",
      "u2 · u3: -0.000\n",
      "\n",
      "======================================================================\n",
      "PART (c): Projection of Price Vector\n",
      "======================================================================\n",
      "Projection of y onto column space: [201.66 301.22 238.2  398.25 351.2 ]\n",
      "\n",
      "Interpretation in context of linear regression:\n",
      "TODO: Explain what this projection represents\n",
      "\n",
      "This projection represents the predicted prices from a linear model using Size, Bedrooms, and Age as features to predict Prices. It's the closest point to the prices in the column space of features. This is essentially the output of a linear regression model fitted to the data.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PART (d): Cosine Similarity\n",
      "======================================================================\n",
      "Cosine similarity between size and bedrooms: 0.9962\n",
      "Angle between vectors: 4.9762°\n",
      "\n",
      "Interpretation:\n",
      "TODO: Explain what this cosine similarity tells us about the relationship\n",
      "between house size and number of bedrooms\n",
      "\"\n",
      "Cosine similarity of 0.9962 indicates very strong positive correlation between Size and bedrooms, this means they are almost perfectly aligned, suggesting multicollinearity. This means as the size of the house increases, the number of bedrooms also tends to increases proportionally and vice versa.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ADDITIONAL ANALYSIS\n",
      "======================================================================\n",
      "Correlation matrix of features:\n",
      "[[1.   0.94 0.95]\n",
      " [0.94 1.   0.96]\n",
      " [0.95 0.96 1.  ]]\n",
      "\n",
      "======================================================================\n",
      "END OF ANALYSIS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# Question 8: Feature Engineering and Dimensionality Reduction\n",
    "# ============================================================================\n",
    "\n",
    "# Given data\n",
    "s = np.array([1000, 1500, 1200, 2000, 1800])  # Size in sq. ft.\n",
    "b = np.array([2, 3, 2, 4, 3])                   # Number of bedrooms\n",
    "a = np.array([5, 10, 2, 20, 15])                # Age in years\n",
    "y = np.array([200, 300, 240, 400, 350])         # Prices in thousands\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GIVEN DATA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Size (s): {s}\")\n",
    "print(f\"Bedrooms (b): {b}\")\n",
    "print(f\"Age (a): {a}\")\n",
    "print(f\"Prices (y): {y}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# Part (a): Design Matrix and Linear Dependence\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"PART (a): Design Matrix and Linear Dependence\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TODO: Create the design matrix X with bias term (column of ones) and features\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "\n",
    "X = np.column_stack([np.ones(5), s, b, a])\n",
    "\n",
    "print(\"Design Matrix X:\")\n",
    "print(X.astype(int))\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print()\n",
    "\n",
    "# TODO: Check for linear dependence\n",
    "rank_X = np.linalg.matrix_rank(X)\n",
    "num_cols = X.shape[1]\n",
    "\n",
    "print(f\"Rank of X: {rank_X}\")\n",
    "print(f\"Number of columns: {num_cols}\")\n",
    "print(f\"Are columns linearly independent? {rank_X == num_cols}\")\n",
    "print()\n",
    "\n",
    "# TODO: Compute determinant of X^T X and its determinant\n",
    "XTX = X.T @ X\n",
    "det_XTX = np.linalg.det(XTX)\n",
    "\n",
    "print(f\"Determinant of X^T X: {det_XTX}\")\n",
    "print()\n",
    "\n",
    "# TODO: Find a non-trivial linear combination (if columns are dependent)\n",
    "print(\"Finding linear dependence relationship...\")\n",
    "if rank_X < num_cols:\n",
    "    U, S, Vt = np.linalg.svd(X)\n",
    "    null_space_vector = Vt[-1, :]\n",
    "    print(f\"Non-trivial linear combination coefficients: {null_space_vector}\")\n",
    "else:\n",
    "    print(\"Columns are linearly independent - no non-trivial linear combination exists\")\n",
    "\n",
    "print()\n",
    "print(\"TODO: Write your interpretation of what linear dependence means for ML models\")\n",
    "print(\"Practical Implication:\")\n",
    "print(\"\"\"\n",
    "Linear dependence means that the coefficient of one feature can be expressed as \n",
    "a combination of others, preventing unique solutions to a linear regression \n",
    "which then leads to infinitely many solutions. Also, models may become unstable \n",
    "and sensitive to small changes in data which can lead to overfitting. This is \n",
    "known as multicollinearity in regression analysis and this can be remedied by \n",
    "removing redundant features or using regularization techniques, or by applying \n",
    "PCA to reduce dimensionality.\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# Part (b): Gram-Schmidt Orthogonalization\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"PART (b): Gram-Schmidt Orthogonalization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract feature vectors (excluding bias term)\n",
    "v1 = s  # Size\n",
    "v2 = b  # Bedrooms\n",
    "v3 = a  # Age\n",
    "\n",
    "print(\"Original feature vectors:\")\n",
    "print(f\"v1 (size): {v1}\")\n",
    "print(f\"v2 (bedrooms): {v2}\")\n",
    "print(f\"v3 (age): {v3}\")\n",
    "print()\n",
    "\n",
    "# TODO: Apply Gram-Schmidt process\n",
    "u1 = v1.copy()\n",
    "\n",
    "proj_u1_v2 = (np.dot(v2, u1) / np.dot(u1, u1)) * u1\n",
    "u2 = v2 - proj_u1_v2\n",
    "\n",
    "proj_u1_v3 = (np.dot(v3, u1) / np.dot(u1, u1)) * u1\n",
    "proj_u2_v3 = (np.dot(v3, u2) / np.dot(u2, u2)) * u2\n",
    "u3 = v3 - proj_u1_v3 - proj_u2_v3\n",
    "\n",
    "# TODO: Normalize to get orthonormal basis\n",
    "q1 = u1 / np.linalg.norm(u1)\n",
    "q2 = u2 / np.linalg.norm(u2)\n",
    "q3 = u3 / np.linalg.norm(u3)\n",
    "\n",
    "print(\"Orthogonal basis vectors:\")\n",
    "print(f\"u1: {u1}\")\n",
    "print(f\"u2: {u2}\")\n",
    "print(f\"u3: {u3}\")\n",
    "print()\n",
    "\n",
    "# TODO: Verify orthogonality by checking dot products\n",
    "print(\"Verification of orthogonality:\")\n",
    "print(f\"u1 · u2: {np.dot(u1, u2):.3f}\")\n",
    "print(f\"u1 · u3: {np.dot(u1, u3):.3f}\")\n",
    "print(f\"u2 · u3: {np.dot(u2, u3):.3f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# Part (c): Projection of Price Vector\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"PART (c): Projection of Price Vector\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TODO: Compute projection of y onto the column space of features\n",
    "y_proj = (np.dot(y, q1) * q1 + \n",
    "          np.dot(y, q2) * q2 + \n",
    "          np.dot(y, q3) * q3)\n",
    "\n",
    "print(f\"Projection of y onto column space: {y_proj}\")\n",
    "print()\n",
    "\n",
    "# TODO: Compute projection matrix and apply to y\n",
    "X_feat = np.column_stack([s, b, a])\n",
    "P = X_feat @ np.linalg.inv(X_feat.T @ X_feat) @ X_feat.T\n",
    "y_proj_matrix = P @ y\n",
    "\n",
    "print(\"Interpretation in context of linear regression:\")\n",
    "print(\"TODO: Explain what this projection represents\")\n",
    "print(\"\"\"\n",
    "This projection represents the predicted prices from a linear model using Size, \n",
    "Bedrooms, and Age as features to predict Prices. It's the closest point to the \n",
    "prices in the column space of features. This is essentially the output of a \n",
    "linear regression model fitted to the data.\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# Part (d): Cosine Similarity\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"PART (d): Cosine Similarity\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TODO: Compute cosine similarity between s and b\n",
    "dot_product_sb = np.dot(s, b)\n",
    "norm_s = np.linalg.norm(s)\n",
    "norm_b = np.linalg.norm(b)\n",
    "cos_similarity = dot_product_sb / (norm_s * norm_b)\n",
    "\n",
    "print(f\"Cosine similarity between size and bedrooms: {cos_similarity:.4f}\")\n",
    "\n",
    "# TODO: Compute the angle in degrees\n",
    "angle_rad = np.arccos(np.clip(cos_similarity, -1.0, 1.0))\n",
    "angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "print(f\"Angle between vectors: {angle_deg:.4f}°\")\n",
    "print()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"TODO: Explain what this cosine similarity tells us about the relationship\")\n",
    "print(\"between house size and number of bedrooms\")\n",
    "print(f\"\"\"\"\n",
    "Cosine similarity of {cos_similarity:.4f} indicates very strong positive \n",
    "correlation between Size and bedrooms, this means they are almost perfectly \n",
    "aligned, suggesting multicollinearity. This means as the size of the house \n",
    "increases, the number of bedrooms also tends to increases proportionally and\n",
    "vice versa.\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# Additional Analysis (Optional)\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ADDITIONAL ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"Correlation matrix of features:\")\n",
    "features = np.column_stack([s, b, a])\n",
    "correlation_matrix = np.corrcoef(features.T)\n",
    "print(correlation_matrix)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"END OF ANALYSIS\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
